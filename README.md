# Synthetic SaaS Review Generation with Quality Guardrails

## Overview

This project implements an end-to-end pipeline for **generating, filtering, scoring, and evaluating synthetic user reviews** for a **generic SaaS task management platform**.

The primary goal is to demonstrate how synthetic data can be produced at scale while maintaining **realism, diversity, and measurable quality**, and how it can be systematically compared against real-world data.

The pipeline covers:

- Multi-model generation (OpenAI & Anthropic)
- Automated quality guardrails
- Per-sample quality scoring
- Dataset-level bias and diversity analysis
- Real vs. synthetic comparison
- Transparent reporting of model performance

---

## Project Structure

```text
SENTIMENT_DATA/
├── analysis/
│   ├── bias_analysis.py
│   ├── real_comparison.py
│   └── report.py
│
├── evaluation/
│   ├── sentiment.py
│   ├── diversity.py
│   └── realism.py
│
├── models/
│   ├── openai_model.py
│   └── anthropic_model.py
│
├── outputs/
│   ├── synthetic_reviews.jsonl
│   ├── synthetic_reviews_scored.jsonl
│   ├── run_log.json
│   └── quality_report.md
│
├── real_data/
│   └── real_reviews.json
│
├── generate.py
├── score_dataset.py
├── config.yaml
├── requirements.txt
└── README.md
```
---

## Data Files
### 1. Synthetic Reviews (Generated)
```bash
File: outputs/synthetic_reviews.jsonl
```
Contains the raw synthetic reviews that passed all quality guardrails during generation.

Each line is a JSON object:

{
  "model": "openai",
  "persona": "QA engineer",
  "rating": 3,
  "review": "..."
}

### 2. Synthetic Reviews with Quality Scores
```bash
File: outputs/synthetic_reviews_scored.jsonl
```
Generated by running score_dataset.py.
This file extends the generated dataset with explicit per-sample quality scores.

{
  "model": "openai",
  "persona": "QA engineer",
  "rating": 3,
  "review": "...",
  "quality_score": 
}

### 3. Real Reviews (Reference Only)
```bash
File: real_data/real_reviews.json
```
A list of real-world SaaS task management reviews collected from public sources (e.g. Asana on G2).

These reviews are used only for statistical comparison, not for generation or training.

--- 

## Setup Instructions
### 1. Requirements

Python 3.10+

CPU-only environment (no GPU required)

### 2. Install Dependencies
pip install -r requirements.txt

### 3. API Keys

Set environment variables:

export OPENAI_API_KEY="your_openai_key"

export ANTHROPIC_API_KEY="your_anthropic_key"

---

## Running the Pipeline
### Step 1 Generate Synthetic Reviews
```bash
python generate.py
```


This step:

- Generates reviews using OpenAI and Anthropic models
- Applies automated quality guardrails
- Rejects low-quality samples
- Produces:
    - synthetic_reviews.jsonl
    - run_log.json
    - quality_report.md

---

### Step 2 Score the Dataset (Post-generation)
```bash
python score_dataset.py
```

This step:
- Re-evaluates accepted reviews
- Computes per-sample quality scores
- Outputs:
    - synthetic_reviews_scored.jsonl

**No regeneration is performed.**

---

## Generation Approach

### Models

- OpenAI (chat models + embeddings)
- Anthropic Claude

Models are randomly selected per attempt to ensure **provider diversity**.

---

### Personas & Ratings

Each review is generated using:

- A randomly selected persona (e.g. QA engineer, Product Manager)
- A probabilistic rating distribution (1–5 stars)
- Persona-specific writing styles

This ensures:

- Role-aware realism
- Linguistic diversity
- Balanced sentiment distribution

---

### Quality Guardrails (During Generation)

Each review must pass all of the following checks:

1. **Sentiment vs Rating Alignment**  
   Prevents mismatches between star rating and textual sentiment.

2. **Domain Realism**  
   Requires mentions of SaaS task management features (e.g. tasks, workflows).

3. **Drawback Requirement for High Ratings**  
   Ensures realistic criticism in 4–5 star reviews.

4. **Semantic Diversity (Embeddings)**  
   Prevents near-duplicate reviews using cosine similarity.

5. **Vocabulary Overlap Control**  
   Limits excessive lexical repetition.

Only reviews that pass **all guardrails** are retained.

---

## Quality Scores

### Definition

Quality scores are computed **after generation** using the same guardrails.

Each review is evaluated across five quality dimensions:

- Sentiment alignment
- Domain realism
- Drawback presence
- Semantic uniqueness
- Vocabulary diversity

The quality score is defined as:

```text
quality_score = passed_checks / total_checks
```
### Result

Since the final dataset contains **only reviews that passed all checks**, all released samples receive:

```text
quality_score = 1.0
```
**Note:** The quality score is intentionally binary in the released dataset.
All samples have passed every quality guardrail and are therefore assigned
a score of 1.0. Rejected samples are excluded entirely rather than retained
with partial scores.

---

## Evaluation & Analysis

### Bias & Distribution Analysis

Implemented in `analysis/bias_analysis.py`:

- Sentiment distribution
- Rating distribution
- Persona distribution

---

### Real vs Synthetic Comparison

Implemented in `analysis/real_comparison.py`:

- Average review length
- Average sentiment polarity
- Positive / negative sentiment ratios

This validates alignment with real-world data without copying content.

---

### Model Performance Tracking

Stored in `outputs/run_log.json`:

- Accepted vs rejected samples per model
- Average generation time per provider

---

### Quality Report

File: `outputs/quality_report.md`

Includes:

- Dataset overview
- Distribution metrics
- Real vs synthetic comparison
- Model performance statistics
- Final conclusions

---

## Design Decisions

### Generic Domain (No Brand Lock-In)

The dataset represents a **generic SaaS task management tool**.

**Rationale:**

- Avoid vendor bias
- Improve generalization
- Prevent overfitting to a single product

---

### Strict Filtering Over Soft Scoring

Low-quality samples are rejected rather than retained with low scores.

**Rationale:**

- Higher dataset reliability
- Cleaner downstream usage
- Industry-aligned data curation practice

---

### CPU-Only Execution

The pipeline is designed to run entirely on CPU.

**Implications:**

- Sequential embedding comparisons
- Lower throughput
- Higher accessibility and reproducibility

---

## Trade-Offs & Limitations

| Trade-Off | Explanation |
|----------|-------------|
| Lower yield | Strict guardrails reject many samples |
| Binary quality scores | Dataset contains only approved samples |
| CPU-only execution | Slower but widely reproducible |
| No brand-specific modeling | Improves generality |

---

## Conclusion

This project demonstrates a **production-style synthetic data pipeline** that emphasizes:

- Quality over quantity
- Explicit evaluation
- Transparent design decisions
- Real-world alignment without data leakage

The result is a **clean, realistic, and well-evaluated synthetic dataset**.
